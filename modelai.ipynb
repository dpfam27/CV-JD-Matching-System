{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from typing import List, Dict, Union, Optional\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/long/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/long/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/long/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 Text Cleaning and Skill Extraction\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters and numbers\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        # Remove stopwords and lemmatize\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token not in self.stop_words]\n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def extract_skills(self, text: str, skills_categories: Dict[str, List[str]]) -> Dict[str, List[str]]:\n",
    "        skills_found = {category: [] for category in skills_categories}\n",
    "        for category, skills in skills_categories.items():\n",
    "            for skill in skills:\n",
    "                pattern = r'\\b' + re.escape(skill.lower()) + r'\\b'\n",
    "                if re.search(pattern, text):\n",
    "                    skills_found[category].append(skill.lower())\n",
    "        return skills_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define Skills Categories\n",
    "skills_categories = {\n",
    "    \"programming\": [\"python\", \"java\", \"c++\", \"javascript\", \"sql\"],\n",
    "    \"data_science\": [\"machine learning\", \"data analysis\", \"numpy\", \"pandas\", \"scikit-learn\"],\n",
    "    \"web_dev\": [\"django\", \"flask\", \"react\", \"angular\", \"html\", \"css\"],\n",
    "    \"cloud\": [\"aws\", \"azure\", \"gcp\", \"docker\", \"kubernetes\"],\n",
    "    \"tools\": [\"git\", \"linux\", \"jupyter\", \"excel\", \"tableau\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCV file not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcv_csv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please check the path.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Read CSV files\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m jd_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjd_csv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m cv_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(cv_csv_path)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Extract text columns (assuming the text columns are named 'text')\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/AAAIIIIIIIII/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/AAAIIIIIIIII/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Downloads/AAAIIIIIIIII/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/AAAIIIIIIIII/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Downloads/AAAIIIIIIIII/.venv/lib/python3.12/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:581\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# 5. Import Your CSV Files\n",
    "jd_csv_path = '/home/long/Downloads/AAAIIIIIIIII/CV-JD-Matching-System/data/raw/jd/train/train.csv'\n",
    "cv_csv_path = '/home/long/Downloads/AAAIIIIIIIII/CV-JD-Matching-System/data/raw/cv/test/test.csv'\n",
    "\n",
    "# Check if files exist\n",
    "if not os.path.exists(jd_csv_path):\n",
    "    print(f\"JD file not found at {jd_csv_path}. Please check the path.\")\n",
    "if not os.path.exists(cv_csv_path):\n",
    "    print(f\"CV file not found at {cv_csv_path}. Please check the path.\")\n",
    "\n",
    "# Read CSV files\n",
    "jd_df = pd.read_csv(jd_csv_path)\n",
    "cv_df = pd.read_csv(cv_csv_path)\n",
    "\n",
    "# Extract text columns (assuming the text columns are named 'text')\n",
    "jd_raw_text = jd_df['text'].iloc[0]  # Adjust the index as needed\n",
    "cv_raw_text = cv_df['text'].iloc[0]  # Adjust the index as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Preprocess the Text\n",
    "# Initialize Preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Clean the texts\n",
    "cleaned_jd = preprocessor.clean_text(jd_raw_text)\n",
    "cleaned_cv = preprocessor.clean_text(cv_raw_text)\n",
    "\n",
    "print(\"Cleaned Job Description:\")\n",
    "print(cleaned_jd[:500])  # Print first 500 characters\n",
    "\n",
    "print(\"\\nCleaned CV:\")\n",
    "print(cleaned_cv[:500])  # Print first 500 characters\n",
    "\n",
    "# Extract Skills\n",
    "jd_skills = preprocessor.extract_skills(cleaned_jd, skills_categories)\n",
    "cv_skills = preprocessor.extract_skills(cleaned_cv, skills_categories)\n",
    "\n",
    "print(\"JD Skills:\", jd_skills)\n",
    "print(\"CV Skills:\", cv_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Preprocess the Text\n",
    "\n",
    "# Initialize Preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Extract text from PDFs\n",
    "jd_raw_text = extract_text_from_pdf(jd_pdf_path)\n",
    "cv_raw_text = extract_text_from_pdf(cv_pdf_path)\n",
    "\n",
    "# Clean the texts\n",
    "cleaned_jd = preprocessor.clean_text(jd_raw_text)\n",
    "cleaned_cv = preprocessor.clean_text(cv_raw_text)\n",
    "\n",
    "print(\"Cleaned Job Description:\")\n",
    "print(cleaned_jd[:500])  # Print first 500 characters\n",
    "\n",
    "print(\"\\nCleaned CV:\")\n",
    "print(cleaned_cv[:500])  # Print first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Build Vocabulary\n",
    "def build_vocab(jd_skills: Dict[str, List[str]], cv_skills: Dict[str, List[str]]) -> Dict[str, int]:\n",
    "    all_skills = []\n",
    "    for category in skills_categories:\n",
    "        all_skills.extend(jd_skills.get(category, []))\n",
    "        all_skills.extend(cv_skills.get(category, []))\n",
    "    vocab = {skill.lower(): idx+1 for idx, skill in enumerate(sorted(set(all_skills)))}\n",
    "    return vocab\n",
    "\n",
    "vocab = build_vocab(jd_skills, cv_skills)\n",
    "print(\"Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Encode Skills into Numerical Format\n",
    "def encode_skills(skills: Dict[str, List[str]], vocab: Dict[str, int]) -> List[int]:\n",
    "    encoded = []\n",
    "    for category in skills_categories:\n",
    "        for skill in skills.get(category, []):\n",
    "            encoded.append(vocab.get(skill.lower(), 0))\n",
    "    return encoded\n",
    "\n",
    "jd_encoded = encode_skills(jd_skills, vocab)\n",
    "cv_encoded = encode_skills(cv_skills, vocab)\n",
    "\n",
    "print(\"Encoded JD Skills:\", jd_encoded)\n",
    "print(\"Encoded CV Skills:\", cv_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Define the RNN Model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.rnn = nn.RNN(embedding_dim * 2, hidden_dim, batch_first=True, nonlinearity='relu')\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, jd, cv):\n",
    "        embedded_jd = self.embedding(jd)  # [batch_size, seq_length, embedding_dim]\n",
    "        embedded_cv = self.embedding(cv)  # [batch_size, seq_length, embedding_dim]\n",
    "        combined = torch.cat((embedded_jd, embedded_cv), dim=2)  # [batch_size, seq_length, embedding_dim*2]\n",
    "        _, hidden = self.rnn(combined)  # hidden: [1, batch_size, hidden_dim]\n",
    "        out = self.fc(hidden.squeeze(0))  # [batch_size, output_dim]\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Create Dataset and DataLoader\n",
    "class SkillsDataset(Dataset):\n",
    "    def __init__(self, jd_skills_list: List[List[int]], cv_skills_list: List[List[int]], labels: List[float], vocab: Dict[str, int]):\n",
    "        self.jd_skills_list = jd_skills_list\n",
    "        self.cv_skills_list = cv_skills_list\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.jd_skills_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.jd_skills_list[idx], dtype=torch.long),\n",
    "            torch.tensor(self.cv_skills_list[idx], dtype=torch.long),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        )\n",
    "\n",
    "# Example with a single sample\n",
    "jd_skills_list = [jd_encoded]  # List of lists\n",
    "cv_skills_list = [cv_encoded]  # List of lists\n",
    "labels = [1.0]  # 1 for a positive match\n",
    "\n",
    "dataset = SkillsDataset(jd_skills_list, cv_skills_list, labels, vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Verify the Data Structures\n",
    "print(\"Type of jd_skills_list:\", type(jd_skills_list))\n",
    "print(\"Type of cv_skills_list:\", type(cv_skills_list))\n",
    "print(\"Type of labels:\", type(labels))\n",
    "\n",
    "print(\"First element in jd_skills_list:\", jd_skills_list[0])\n",
    "print(\"First element in cv_skills_list:\", cv_skills_list[0])\n",
    "print(\"First label:\", labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Initialize Model, Loss, and Optimizer\n",
    "vocab_size = len(vocab) + 1  # +1 for padding_idx=0\n",
    "embedding_dim = 100\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "\n",
    "model = RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Training Loop with Early Stopping\n",
    "epochs = 50\n",
    "patience = 5\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_model_path = 'best_model.pth'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for jd, cv, label in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(jd, cv)\n",
    "        \n",
    "        # Ensure labels have shape [batch_size, 1]\n",
    "        label = label.unsqueeze(1)\n",
    "        \n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    # Early Stopping\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        counter = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Load the Best Model\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Compute Matching Percentage and Analyze Missing Skills\n",
    "def compute_matching_percentage(jd_skills: Dict[str, List[str]], cv_skills: Dict[str, List[str]]) -> (float, List[str]):\n",
    "    jd_set = set()\n",
    "    cv_set = set()\n",
    "    for category in skills_categories:\n",
    "        jd_set.update(jd_skills.get(category, []))\n",
    "        cv_set.update(cv_skills.get(category, []))\n",
    "    intersection = jd_set.intersection(cv_set)\n",
    "    percentage = (len(intersection) / len(jd_set)) * 100 if jd_set else 0\n",
    "    missing_skills = list(jd_set - cv_set)\n",
    "    return percentage, missing_skills\n",
    "\n",
    "matching_percentage, missing_skills = compute_matching_percentage(jd_skills, cv_skills)\n",
    "print(f\"Matching Percentage: {matching_percentage:.2f}%\")\n",
    "print(f\"Missing Skills: {missing_skills}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Recommend Courses Based on Missing Skills\n",
    "def recommend_courses(missing_skills: List[str]) -> List[str]:\n",
    "    course_mapping = {\n",
    "        'python': 'Python for Everybody',\n",
    "        'machine learning': 'Introduction to Machine Learning',\n",
    "        'data analysis': 'Data Analysis with Pandas',\n",
    "        'java': 'Java Programming Masterclass',\n",
    "        'javascript': 'JavaScript Essentials',\n",
    "        'c++': 'C++ Fundamentals',\n",
    "        'django': 'Django for Beginners',\n",
    "        'flask': 'Flask Web Development',\n",
    "        'react': 'React - The Complete Guide',\n",
    "        'aws': 'AWS Certified Solutions Architect',\n",
    "        'docker': 'Docker for Developers',\n",
    "        'git': 'Version Control with Git',\n",
    "        'sql': 'SQL for Data Science',\n",
    "        'numpy': 'Numerical Computing with NumPy',\n",
    "        'pandas': 'Data Manipulation with Pandas',\n",
    "        'scikit-learn': 'Machine Learning with scikit-learn',\n",
    "        'html': 'HTML & CSS Essentials',\n",
    "        'css': 'CSS - The Complete Guide',\n",
    "        'angular': 'Angular - The Complete Guide',\n",
    "        'gcp': 'Google Cloud Platform Fundamentals',\n",
    "        'kubernetes': 'Kubernetes for Developers',\n",
    "        'linux': 'Linux Administration Bootcamp',\n",
    "        'jupyter': 'Jupyter Notebook for Data Science',\n",
    "        'excel': 'Mastering Excel for Data Analysis',\n",
    "        'tableau': 'Tableau for Data Visualization'\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "    recommended = [course_mapping.get(skill.lower(), f\"Course for {skill}\") for skill in missing_skills]\n",
    "    return recommended\n",
    "\n",
    "recommendations = recommend_courses(missing_skills)\n",
    "print(\"\\nRecommended Courses:\")\n",
    "for course in recommendations:\n",
    "    print(f\"- {course}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Save the Model as Pickle\n",
    "# Note: Using torch.save is recommended, but here we use pickle as per your requirement\n",
    "with open('rnn_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"\\nModel saved as 'rnn_model.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Evaluate Precision and Recall\n",
    "# Dummy true labels\n",
    "true_labels = [1.0]  # Assuming it's a positive match\n",
    "\n",
    "# Model predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(\n",
    "        torch.tensor(jd_encoded, dtype=torch.long).unsqueeze(0),\n",
    "        torch.tensor(cv_encoded, dtype=torch.long).unsqueeze(0)\n",
    "    )\n",
    "    predicted_labels = (predictions.squeeze() > 0.5).int().tolist()\n",
    "\n",
    "print(f\"\\nPredicted Labels: {predicted_labels}\")\n",
    "\n",
    "# Calculate Precision and Recall\n",
    "precision = precision_score(true_labels, predicted_labels, zero_division=0)\n",
    "recall = recall_score(true_labels, predicted_labels, zero_division=0)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Evaluate Precision and Recall\n",
    "# Dummy true labels\n",
    "true_labels = [1.0]  # Assuming it's a positive match\n",
    "\n",
    "# Model predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(\n",
    "        torch.tensor(jd_encoded, dtype=torch.long).unsqueeze(0),\n",
    "        torch.tensor(cv_encoded, dtype=torch.long).unsqueeze(0)\n",
    "    )\n",
    "    predicted_labels = (predictions.squeeze() > 0.5).int().tolist()\n",
    "\n",
    "print(f\"\\nPredicted Labels: {predicted_labels}\")\n",
    "\n",
    "# Calculate Precision and Recall\n",
    "precision = precision_score(true_labels, predicted_labels, zero_division=0)\n",
    "recall = recall_score(true_labels, predicted_labels, zero_division=0)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Save the Model as Pickle\n",
    "# Note: Using torch.save is recommended, but here we use pickle as per your requirement\n",
    "with open('rnn_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "print(\"\\nModel saved as 'rnn_model.pkl'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
